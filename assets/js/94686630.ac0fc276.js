"use strict";(self.webpackChunkredis_developer_hub=self.webpackChunkredis_developer_hub||[]).push([[2925],{3905:function(e,t,n){n.d(t,{Zo:function(){return u},kt:function(){return m}});var i=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)n=r[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=i.createContext({}),c=function(e){var t=i.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=c(e.components);return i.createElement(l.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},d=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=c(n),m=a,f=d["".concat(l,".").concat(m)]||d[m]||p[m]||r;return n?i.createElement(f,o(o({ref:t},u),{},{components:n})):i.createElement(f,o({ref:t},u))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=d;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,o[1]=s;for(var c=2;c<r;c++)o[c]=n[c];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}d.displayName="MDXCreateElement"},98781:function(e,t,n){n.r(t),n.d(t,{assets:function(){return u},contentTitle:function(){return l},default:function(){return m},frontMatter:function(){return s},metadata:function(){return c},toc:function(){return p}});var i=n(87462),a=n(63366),r=(n(67294),n(3905)),o=(n(44996),["components"]),s={id:"index-initial-tuning",title:"1.5 Initial Tuning",sidebar_label:"1.5 Initial Tuning",slug:"/operate/redis-at-scale/talking-to-redis/initial-tuning",custom_edit_url:null},l=void 0,c={unversionedId:"operate/redis-at-scale/talking-to-redis/initial-tuning/index-initial-tuning",id:"operate/redis-at-scale/talking-to-redis/initial-tuning/index-initial-tuning",title:"1.5 Initial Tuning",description:"We love Redis because it\u2019s fast (and fun!), so as we begin to consider scaling out Redis, we first want to make sure we've done everything we can to maximize its performance.",source:"@site/docs/operate/redis-at-scale/talking-to-redis/initial-tuning/index-initial-tuning.mdx",sourceDirName:"operate/redis-at-scale/talking-to-redis/initial-tuning",slug:"/operate/redis-at-scale/talking-to-redis/initial-tuning",permalink:"/operate/redis-at-scale/talking-to-redis/initial-tuning",draft:!1,editUrl:null,tags:[],version:"current",lastUpdatedAt:1655485542,formattedLastUpdatedAt:"6/17/2022",frontMatter:{id:"index-initial-tuning",title:"1.5 Initial Tuning",sidebar_label:"1.5 Initial Tuning",slug:"/operate/redis-at-scale/talking-to-redis/initial-tuning",custom_edit_url:null},sidebar:"docs",previous:{title:"1.4 Client Performance Improvements",permalink:"/operate/redis-at-scale/talking-to-redis/client-performance-improvements"},next:{title:"2.0 Introduction to Persistence and Durability'",permalink:"/operate/redis-at-scale/persistence-and-durability/introduction"}},u={},p=[{value:"Max Clients",id:"max-clients",level:2},{value:"Max Memory",id:"max-memory",level:2},{value:"Set TCP-BACKLOG",id:"set-tcp-backlog",level:2},{value:"Set Read Replica Configurations",id:"set-read-replica-configurations",level:2},{value:"Kernel Memory",id:"kernel-memory",level:2},{value:"Kernel Network Stack",id:"kernel-network-stack",level:2},{value:"File Descriptor Limits",id:"file-descriptor-limits",level:2},{value:"Enabling RPS (Receive Packet Steering) and CPU preferences",id:"enabling-rps-receive-packet-steering-and-cpu-preferences",level:2}],d={toc:p};function m(e){var t=e.components,n=(0,a.Z)(e,o);return(0,r.kt)("wrapper",(0,i.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"We love Redis because it\u2019s fast (and fun!), so as we begin to consider scaling out Redis, we first want to make sure we've done everything we can to maximize its performance."),(0,r.kt)("p",null,"Let's start by looking at some important tuning parameters."),(0,r.kt)("h2",{id:"max-clients"},"Max Clients"),(0,r.kt)("p",null,"Redis has a default of max of 10,000 clients; after that maximum has been reached, Redis will respond to all new connections with an error. If you have a lot of connections (or a lot of application instances), then you may need to go higher. You can set the max number of simultaneous clients in the Redis config file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"maxclients 20000\n")),(0,r.kt)("h2",{id:"max-memory"},"Max Memory"),(0,r.kt)("p",null,"By default, Redis has no max memory limit, so it will use all available system memory. If you are using replication, you will want to limit the memory usage in order to have overhead for replica output buffers. It\u2019s also a good idea to leave memory for the system. Something like 25% overhead. You can update this setting in Redis config file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# memory size in bytes\nmaxmemory 1288490188\n")),(0,r.kt)("h2",{id:"set-tcp-backlog"},"Set TCP-BACKLOG"),(0,r.kt)("p",null,"The Redis server uses the value of ",(0,r.kt)("inlineCode",{parentName:"p"},"tcp-backlog")," to specify the size of the complete connection queue."),(0,r.kt)("p",null,"Redis passes this configuration as the second parameter of the ",(0,r.kt)("inlineCode",{parentName:"p"},"listen(int s, int backlog)")," call."),(0,r.kt)("p",null,"If you have many connections, you will need to set this higher than the default of 511. You can update this in Redis config file:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# TCP listen() backlog.\n#\n# In high requests-per-second environments you need an high backlog in order\n# to avoid slow clients connections issues. Note that the Linux kernel\n# will silently truncate it to the value of /proc/sys/net/core/somaxconn so\n# make sure to raise both the value of somaxconn and tcp_max_syn_backlog\n# in order to get the desired effect.\ntcp-backlog 65536\n")),(0,r.kt)("p",null,"As the comment in ",(0,r.kt)("inlineCode",{parentName:"p"},"redis.conf")," indicates, the value of ",(0,r.kt)("inlineCode",{parentName:"p"},"somaxconn")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"tcp_max_syn_backlog")," may need to be increased at the OS level as well."),(0,r.kt)("h2",{id:"set-read-replica-configurations"},"Set Read Replica Configurations"),(0,r.kt)("p",null,"One simple way to scale Redis is to add read replicas and take load off of the primary. This is most effective when you have a read-heavy (as opposed to write-heavy) workload. You will probably want to have the replica available and still serving stale data, even if the replication is not completed. You can update this in the Redis config:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"slave-serve-stale-data yes\n")),(0,r.kt)("p",null,"You will also want to prevent any writes from happening on the replicas. You can update this in the Redis config:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"slave-read-only yes\n")),(0,r.kt)("h2",{id:"kernel-memory"},"Kernel Memory"),(0,r.kt)("p",null,"Under high load, occasional performance dips can occur due to memory allocation. This is something Salvatore, the creator of Redis, blogged about in the past. The performance issue is related to transparent hugepages, which you can disable at the OS level if needed."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ echo 'never' > /sys/kernel/mm/transparent_hugepage/enabled\n")),(0,r.kt)("h2",{id:"kernel-network-stack"},"Kernel Network Stack"),(0,r.kt)("p",null,"If you plan on handling a large number of connections in a high performance environment, we recommend tuning the following kernel parameters:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"vm.swappiness=0                       # turn off swapping\nnet.ipv4.tcp_sack=1                   # enable selective acknowledgements\nnet.ipv4.tcp_timestamps=1             # needed for selective acknowledgements\nnet.ipv4.tcp_window_scaling=1         # scale the network window\nnet.ipv4.tcp_congestion_control=cubic # better congestion algorithm\nnet.ipv4.tcp_syncookies=1             # enable syn cookies\nnet.ipv4.tcp_tw_recycle=1             # recycle sockets quickly\nnet.ipv4.tcp_max_syn_backlog=NUMBER   # backlog setting\nnet.core.somaxconn=NUMBER             # up the number of connections per port\nnet.core.rmem_max=NUMBER              # up the receive buffer size\nnet.core.wmem_max=NUMBER              # up the buffer size for all connections\n")),(0,r.kt)("h2",{id:"file-descriptor-limits"},"File Descriptor Limits"),(0,r.kt)("p",null,"If you do not set the correct number of file descriptors for the Redis user, you will see errors indicating that \u201cRedis can\u2019t set maximum open files..\u201d You can increase the file descriptor limit at the OS level."),(0,r.kt)("p",null,"Here's an example on Ubuntu using systemd:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"/etc/systemd/system/redis.service\n[Service]\n...\nUser=redis\nGroup=redis\n...\nLimitNOFILE=65536\n...\n")),(0,r.kt)("p",null,"You will then need to reload the daemon and restart the redis service."),(0,r.kt)("h2",{id:"enabling-rps-receive-packet-steering-and-cpu-preferences"},"Enabling RPS (Receive Packet Steering) and CPU preferences"),(0,r.kt)("p",null,"One way we can improve performance is to prevent Redis from running on the same CPUs as those handling any network traffic. This can be accomplished by enabling RPS for our network interfaces and creating some CPU affinity for our Redis process."),(0,r.kt)("p",null,"Here is an example. First we can enable RPS on CPUs 0-1:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ echo '3' > /sys/class/net/eth1/queues/rx-0/rps_cpus\n")),(0,r.kt)("p",null,"Then we can set the CPU affinity for redis to CPUs 2-8:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"# config is set to write pid to /var/run/redis.pid\n$ taskset -pc 2-8 `cat /var/run/redis.pid`\npid 8946's current affinity list: 0-8\npid 8946's new affinity list: 2-8\n")))}m.isMDXComponent=!0}}]);